#!/usr/bin/env python3
"""
SciVQA Metrics Calculator

This script calculates ROUGE and BERTScore metrics from a JSON results file 
generated by the SciVQA evaluation script.
"""

import argparse
import json
import os
from pathlib import Path
import numpy as np
from tqdm import tqdm
import wandb
from rouge_score import rouge_scorer
from bert_score import score as bert_score


def rouge(predictions, references, rouge_type="rouge1"):
    """Calculate ROUGE scores"""
    scorer = rouge_scorer.RougeScorer([rouge_type], use_stemmer=True)
    scores = []
    
    # Calculate scores for each prediction-reference pair
    for pred, ref in zip(predictions, references):
        score = scorer.score(ref, pred)
        scores.append(score[rouge_type])
    
    # Calculate average scores
    precision = np.mean([score.precision for score in scores])
    recall = np.mean([score.recall for score in scores])
    f1 = np.mean([score.fmeasure for score in scores])
    
    return f1, precision, recall


def bertS(predictions, references):
    """Calculate BERTScore"""
    try:
        # Calculate BERTScore
        P, R, F1 = bert_score(predictions, references, lang="en", verbose=False)
        
        # Convert from torch tensors to float
        precision = float(P.mean())
        recall = float(R.mean())
        f1 = float(F1.mean())
        
        return f1, precision, recall
    except Exception as e:
        print(f"Error calculating BERTScore: {e}")
        return 0.0, 0.0, 0.0


def calculate_metrics_batch(predictions, references):
    """Calculate all metrics for a batch of predictions and references"""
    # Calculate ROUGE-1 scores
    rouge1_f1, rouge1_precision, rouge1_recall = rouge(
        predictions, references, "rouge1")
    
    # Calculate ROUGE-L scores
    rougeL_f1, rougeL_precision, rougeL_recall = rouge(
        predictions, references, "rougeL")
    
    # Calculate BERTScore
    bert_f1, bert_precision, bert_recall = bertS(
        predictions, references)
    
    # Return all metrics
    return {
        "rouge1_f1": rouge1_f1,
        "rouge1_precision": rouge1_precision,
        "rouge1_recall": rouge1_recall,
        "rougeL_f1": rougeL_f1,
        "rougeL_precision": rougeL_precision,
        "rougeL_recall": rougeL_recall,
        "bert_f1": bert_f1,
        "bert_precision": bert_precision,
        "bert_recall": bert_recall
    }


def main():
    parser = argparse.ArgumentParser(
        description="Calculate ROUGE and BERTScore metrics from SciVQA evaluation results"
    )
    parser.add_argument("--input_file", type=str, required=True,
                        help="Input JSON file with evaluation results")
    parser.add_argument("--output_file", type=str, default=None,
                        help="Output JSON file for metrics (default: input_file with '_metrics' suffix)")
    parser.add_argument("--wandb", action="store_true", 
                        help="Log metrics to Weights & Biases")
    parser.add_argument("--wandb_project", type=str, default="scivqa-metrics",
                        help="W&B project name")
    parser.add_argument("--wandb_run_name", type=str, default=None,
                        help="W&B run name (default: derived from input filename)")
    parser.add_argument("--batch_size", type=int, default=100,
                        help="Batch size for processing large files")
    parser.add_argument("--filter_empty", action="store_true",
                        help="Filter out examples with empty predictions or references")
    parser.add_argument("--filter_min_length", type=int, default=0,
                        help="Filter out examples with predictions or references shorter than this")
    parser.add_argument("--by_category", action="store_true",
                        help="Calculate metrics by question and figure type")
    
    args = parser.parse_args()
    
    # Set default output file if not provided
    if not args.output_file:
        input_path = Path(args.input_file)
        args.output_file = str(input_path.parent / f"{input_path.stem}_metrics.json")
    
    print(f"Loading results from {args.input_file}...")
    
    try:
        with open(args.input_file, 'r') as f:
            data = json.load(f)
    except Exception as e:
        print(f"Error loading results file: {e}")
        return
    
    # Check if the file has the expected structure
    if 'results' not in data:
        print("The input file doesn't have a 'results' key. Is this a valid SciVQA results file?")
        return
    
    results = data['results']
    print(f"Loaded {len(results)} examples from results file")
    
    # Initialize W&B if requested
    if args.wandb:
        run_name = args.wandb_run_name or f"metrics-{Path(args.input_file).stem}"
        wandb.init(project=args.wandb_project, name=run_name, config={
            "input_file": args.input_file,
            "num_examples": len(results),
            "filter_empty": args.filter_empty,
            "filter_min_length": args.filter_min_length
        })
    
    # Extract predictions and references
    predictions = []
    references = []
    
    # Track filtered examples
    filtered_count = 0
    
    for result in results:
        pred = result.get('generated', '').strip().lower()
        ref = result.get('answer', '').strip().lower()
        
        # Apply filters if requested
        if args.filter_empty and (not pred or not ref):
            filtered_count += 1
            continue
            
        if args.filter_min_length > 0 and (len(pred) < args.filter_min_length or len(ref) < args.filter_min_length):
            filtered_count += 1
            continue
        
        predictions.append(pred)
        references.append(ref)
    
    if filtered_count > 0:
        print(f"Filtered out {filtered_count} examples based on criteria")
    
    print(f"Calculating metrics for {len(predictions)} examples...")
    
    # Calculate overall metrics
    metrics = calculate_metrics_batch(predictions, references)
    
    # Calculate accuracy
    correct = sum(1 for r in results if r.get('correct', False))
    total = len(results)
    accuracy = correct / total if total > 0 else 0
    metrics['accuracy'] = accuracy
    
    # Print overall metrics
    print("\n=== Overall Metrics ===")
    print(f"Accuracy: {accuracy:.4f} ({correct}/{total})")
    print(f"ROUGE-1 F1: {metrics['rouge1_f1']:.4f}")
    print(f"ROUGE-L F1: {metrics['rougeL_f1']:.4f}")
    print(f"BERTScore F1: {metrics['bert_f1']:.4f}")
    
    # Calculate metrics by category if requested
    category_metrics = {}
    
    if args.by_category:
        print("\nCalculating metrics by category...")
        
        # Group examples by question type
        question_types = {}
        for result in results:
            q_type = result.get('question_type', 'unknown')
            if q_type not in question_types:
                question_types[q_type] = {
                    'predictions': [],
                    'references': [],
                    'correct': 0,
                    'total': 0
                }
            
            question_types[q_type]['total'] += 1
            if result.get('correct', False):
                question_types[q_type]['correct'] += 1
                
            question_types[q_type]['predictions'].append(result.get('generated', '').strip().lower())
            question_types[q_type]['references'].append(result.get('answer', '').strip().lower())
        
        # Group examples by figure type
        figure_types = {}
        for result in results:
            f_type = result.get('figure_type', 'unknown')
            if f_type not in figure_types:
                figure_types[f_type] = {
                    'predictions': [],
                    'references': [],
                    'correct': 0,
                    'total': 0
                }
            
            figure_types[f_type]['total'] += 1
            if result.get('correct', False):
                figure_types[f_type]['correct'] += 1
                
            figure_types[f_type]['predictions'].append(result.get('generated', '').strip().lower())
            figure_types[f_type]['references'].append(result.get('answer', '').strip().lower())
        
        # Calculate metrics for each category
        category_metrics['question_types'] = {}
        for q_type, values in tqdm(question_types.items(), desc="Question types"):
            if values['total'] > 0:
                try:
                    type_metrics = calculate_metrics_batch(
                        values['predictions'], values['references'])
                    type_metrics['accuracy'] = values['correct'] / values['total']
                    
                    category_metrics['question_types'][q_type] = type_metrics
                    
                    print(f"\nQuestion Type: {q_type}")
                    print(f"Accuracy: {type_metrics['accuracy']:.4f} ({values['correct']}/{values['total']})")
                    print(f"ROUGE-1 F1: {type_metrics['rouge1_f1']:.4f}")
                    print(f"ROUGE-L F1: {type_metrics['rougeL_f1']:.4f}")
                    print(f"BERTScore F1: {type_metrics['bert_f1']:.4f}")
                except Exception as e:
                    print(f"Error calculating metrics for question type {q_type}: {e}")
        
        category_metrics['figure_types'] = {}
        for f_type, values in tqdm(figure_types.items(), desc="Figure types"):
            if values['total'] > 0:
                try:
                    type_metrics = calculate_metrics_batch(
                        values['predictions'], values['references'])
                    type_metrics['accuracy'] = values['correct'] / values['total']
                    
                    category_metrics['figure_types'][f_type] = type_metrics
                    
                    print(f"\nFigure Type: {f_type}")
                    print(f"Accuracy: {type_metrics['accuracy']:.4f} ({values['correct']}/{values['total']})")
                    print(f"ROUGE-1 F1: {type_metrics['rouge1_f1']:.4f}")
                    print(f"ROUGE-L F1: {type_metrics['rougeL_f1']:.4f}")
                    print(f"BERTScore F1: {type_metrics['bert_f1']:.4f}")
                except Exception as e:
                    print(f"Error calculating metrics for figure type {f_type}: {e}")
    
    # Create output dictionary
    output = {
        'overall': metrics,
        'categories': category_metrics,
        'input_file': args.input_file,
        'num_examples': len(results),
        'filtered_examples': filtered_count
    }
    
    # Save output
    print(f"\nSaving metrics to {args.output_file}...")
    with open(args.output_file, 'w') as f:
        json.dump(output, f, indent=2)
    
    # Log to W&B if requested
    if args.wandb:
        print("Logging metrics to W&B...")
        
        # Log overall metrics
        wandb.log({
            "accuracy": metrics['accuracy'],
            "rouge1_f1": metrics['rouge1_f1'],
            "rouge1_precision": metrics['rouge1_precision'],
            "rouge1_recall": metrics['rouge1_recall'],
            "rougeL_f1": metrics['rougeL_f1'],
            "rougeL_precision": metrics['rougeL_precision'],
            "rougeL_recall": metrics['rougeL_recall'],
            "bert_f1": metrics['bert_f1'],
            "bert_precision": metrics['bert_precision'],
            "bert_recall": metrics['bert_recall']
        })
        
        # Log category metrics if calculated
        if category_metrics:
            # Create tables for category metrics
            question_type_table = wandb.Table(columns=["Question Type", "Count", "Accuracy", "ROUGE-1 F1", "ROUGE-L F1", "BERTScore F1"])
            for q_type, q_metrics in category_metrics.get('question_types', {}).items():
                question_type_table.add_data(
                    q_type,
                    question_types[q_type]['total'],
                    q_metrics['accuracy'],
                    q_metrics['rouge1_f1'],
                    q_metrics['rougeL_f1'],
                    q_metrics['bert_f1']
                )
            
            figure_type_table = wandb.Table(columns=["Figure Type", "Count", "Accuracy", "ROUGE-1 F1", "ROUGE-L F1", "BERTScore F1"])
            for f_type, f_metrics in category_metrics.get('figure_types', {}).items():
                figure_type_table.add_data(
                    f_type,
                    figure_types[f_type]['total'],
                    f_metrics['accuracy'],
                    f_metrics['rouge1_f1'],
                    f_metrics['rougeL_f1'],
                    f_metrics['bert_f1']
                )
            
            # Log tables
            wandb.log({
                "question_types": question_type_table,
                "figure_types": figure_type_table
            })
            
            # Create bar charts for metrics by category
            metrics_list = [
                ("accuracy", "Accuracy"),
                ("rouge1_f1", "ROUGE-1 F1"),
                ("rougeL_f1", "ROUGE-L F1"),
                ("bert_f1", "BERTScore F1")
            ]
            
            for metric_key, metric_name in metrics_list:
                # Question types chart
                q_type_data = [[q_type, q_metrics[metric_key]] 
                              for q_type, q_metrics in category_metrics.get('question_types', {}).items()]
                
                if q_type_data:
                    wandb.log({
                        f"question_type_{metric_key}": wandb.plot.bar(
                            wandb.Table(data=q_type_data, columns=["Question Type", metric_name]),
                            "Question Type",
                            metric_name,
                            title=f"{metric_name} by Question Type"
                        )
                    })
                
                # Figure types chart
                f_type_data = [[f_type, f_metrics[metric_key]] 
                              for f_type, f_metrics in category_metrics.get('figure_types', {}).items()]
                
                if f_type_data:
                    wandb.log({
                        f"figure_type_{metric_key}": wandb.plot.bar(
                            wandb.Table(data=f_type_data, columns=["Figure Type", metric_name]),
                            "Figure Type",
                            metric_name,
                            title=f"{metric_name} by Figure Type"
                        )
                    })
        
        # Save output file as an artifact
        metrics_artifact = wandb.Artifact(
            name=f"metrics-{Path(args.output_file).stem}",
            type="metrics",
            description=f"Metrics calculated from {args.input_file}"
        )
        metrics_artifact.add_file(args.output_file)
        wandb.log_artifact(metrics_artifact)
        
        # Finish W&B run
        wandb.finish()
    
    print(f"Done! Metrics saved to {args.output_file}")


if __name__ == "__main__":
    main()
